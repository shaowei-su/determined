determined.pytorch
==================

.. _pytorch-trial:

``determined.pytorch.PyTorchTrial``
-----------------------------------

.. autoclass:: determined.pytorch.PyTorchTrial
    :members:
    :exclude-members: trial_controller_class
    :inherited-members:
    :member-order: bysource
    :special-members: __init__

.. autoclass:: determined.pytorch.LRScheduler
    :members:
    :special-members: __init__

.. autoclass:: determined.pytorch.Reducer
    :members:

.. autoclass:: determined.pytorch.PyTorchCallback
    :members:

.. _pytorch-data-loading:

Data Loading
~~~~~~~~~~~~

Loading data into ``PyTorchTrial`` models is done by defining two functions,
``build_training_data_loader()`` and ``build_validation_data_loader()``.
These functions should each return an instance of
``determined.pytorch.DataLoader``.  ``determined.pytorch.DataLoader`` behaves
the same as ``torch.utils.data.DataLoader`` and is a drop-in replacement.

Each ``DataLoader`` is allowed to return batches with arbitrary
structures of the following types, which will be fed directly to the
``train_batch`` and ``evaluate_batch`` functions:

-  ``np.ndarray``

   .. code:: python

      np.array([[0, 0], [0, 0]])

-  ``torch.Tensor``

   .. code:: python

      torch.Tensor([[0, 0], [0, 0]])

-  tuple of ``np.ndarray``\ s or ``torch.Tensor``\ s

   .. code:: python

      (torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]]))

-  list of ``np.ndarray``\ s or ``torch.Tensor``\ s

   .. code:: python

      [torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]])]

-  dictionary mapping strings to ``np.ndarray``\ s or
   ``torch.Tensor``\ s

   .. code:: python

      {"data": torch.Tensor([[0, 0], [0, 0]]), "label": torch.Tensor([[1, 1], [1, 1]])}

-  combination of the above

   .. code:: python

      {
          "data": [
              {"sub_data1": torch.Tensor([[0, 0], [0, 0]])},
              {"sub_data2": torch.Tensor([0, 0])},
          ],
          "label": (torch.Tensor([0, 0]), torch.Tensor([[0, 0], [0, 0]])),
      }

.. _pytorch-trial-context:

Trial Context
~~~~~~~~~~~~~

``determined.pytorch.PyTorchTrialContext`` subclasses :ref:`trial-context`.
It provides useful methods for writing ``Trial`` subclasses.

.. autoclass:: determined.pytorch.PyTorchTrialContext
    :members:

.. _pytorch-callbacks:

Callbacks
~~~~~~~~~

To execute arbitrary Python functionality during the lifecycle of a
``PyTorchTrial``, implement the callback interface:

.. autoclass:: determined.pytorch.PyTorchCallback
    :members:

``ReduceLROnPlateau``
^^^^^^^^^^^^^^^^^^^^^

To use the `torch.optim.lr_scheduler.ReduceLROnPlateau
<https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau>`_
class with ``PyTorchTrial``, implement the following callback:

.. code::

    class ReduceLROnPlateauEveryValidationStep(PyTorchCallback):
        def __init__(self, context):
            self.reduce_lr = torch.optim.lr_scheduler.ReduceLROnPlateau(
                context.get_optimizer(), "min", verbose=True
            )  # customize arguments as desired here

        def on_validation_step_end(self, metrics):
            self.reduce_lr.step(metrics["validation_error"])

        def state_dict(self):
            return self.reduce_lr.state_dict()

        def load_state_dict(self, state_dict):
            self.reduce_lr.load_state_dict(state_dict)

Then, implement the ``build_callbacks`` function in ``PyTorchTrial``:

.. code::

    def build_callbacks(self):
        return {"reduce_lr": ReduceLROnPlateauEveryValidationStep(self.context)}


Gradient Clipping
^^^^^^^^^^^^^^^^^

To perform gradient clipping Determined provides two pre-made callback classes:

.. autoclass:: determined.pytorch.ClipGradsL2Norm
    :members:

.. autoclass:: determined.pytorch.ClipGradsL2Value
    :members:


Migration guide
~~~~~~~~~~~~~~~

As illustrated in `PytorchTrial <determined.pytorch.PyTorchTrial_>`_ , we introduce a
new method to define a Pytorch trial by initializing models, optimizers,
and LR schedulers in the ``__init__`` and customizing training loop in ``train_batch``
by manually running forward and backward passes.
Compared to the previous method that returns those objects in the implemented abstract
methods, the new method is flexible and transparent. To migrate your implementation of
`PytorchTrial <determined.pytorch.PyTorchTrial_>`_ from the previous method,
here are the following places you need to change in your code:

1. Wrap models, optimizers, and LR schedulers in the ``__init__`` with the methods
   ``Model``, ``Optimizer``, and ``LRScheduler`` that are provided by
   `PyTorchTrialContext <pytorch-trial-context_>`_. At the same time,
   remove the implementation of ``build_model``, ``optimizer``, and ``create_lr_scheduler``.
2. If using automatic mixed precision (AMP), configure Apex AMP in the ``__init__``
   with the method ``configure_apex_amp`` that is provided by
   `PyTorchTrialContext <pytorch-trial-context_>`_.
   At the same time, remove the experiment configuration ``optimizations.mixed_precision``.
3. Run backward passes on losses and step optimizers in ``train_batch`` with the methods
   ``backward`` and ``step_optimizer`` provided by `PyTorchTrialContext <pytorch-trial-context_>`_.
   Clip graidents by passing a function to the ``clip_grads`` argument of ``step_optimizer``
   while removing the ``PytorchCallback`` counterpart in ``build_callbacks``.

Examples
--------

-  `cifar10_cnn_pytorch <https://github.com/determined-ai/determined/tree/master/examples/official/trial/cifar10_cnn_pytorch>`_
   (PyTorch ``Sequential`` model)
-  `mnist_pytorch <https://github.com/determined-ai/determined/tree/master/examples/official/trial/mnist_pytorch>`_
   (two examples: PyTorch ``Sequential`` model and true multi-input
   multi-output model)
